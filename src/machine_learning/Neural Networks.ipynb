{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle : In this section we'll focus on one specific type of Deep Learning algorithm, namely multilayer perceptrons. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.\n",
    "This model has a lot more coefficients (also called weights) to learn: there is one between every input and every hidden unit (which make up the hidden layer), and one between every unit in the hidden layer and the output.\n",
    "After computing a weighted sum for each hidden unit, a nonlinear function is applied to the result, usually the rectifying nonlinearity (also known as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The relu cuts off values below zero, while tanh saturates to –1 for low input values and +1 for high input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from utils import feature_selection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.958\n",
      "Accuracy on test set: 0.724\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.01.13-14.25.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, max_iter=10000)\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the results, we rather be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver for weight optimization.\n",
    "- ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "- ‘sgd’ refers to stochastic gradient descent.\n",
    "- ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver : lbfgs\n",
      "Accuracy on training set: 0.859\n",
      "Accuracy on test set: 0.843\n",
      "Solver : sgd\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "Solver : adam\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n"
     ]
    }
   ],
   "source": [
    "solver = ['lbfgs','sgd', 'adam']\n",
    "for i in solver:\n",
    "    print(\"Solver : %s\" % i)\n",
    "    mlp = MLPClassifier(solver=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the 'adam' algorithm performs quite well on our dataset. Still, the performance on the test set might be improved by tuning other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the hidden layer.\n",
    "- ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "- ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "- ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "- ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.877\n",
      "Accuracy on test set: 0.875\n",
      "function : logistic\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : tanh\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : relu\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n"
     ]
    }
   ],
   "source": [
    "act = ['identity','logistic', 'tanh', 'relu']\n",
    "for i in act:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(activation=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the 'identity' activation didn't manage to make the algorithm converge. For the others, both 'logistic' and 'tanh' provided sam results and performed better on the test set than the training set, which is the opposite for the 'relu' activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate schedule for weight updates (only for 'sgd' solver).\n",
    "- ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "- ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : constant\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "function : invscaling\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n",
      "function : adaptive\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.899\n"
     ]
    }
   ],
   "source": [
    "learning_rate = ['constant','invscaling', 'adaptive']\n",
    "for i in learning_rate:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(solver='sgd', learning_rate=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sgd' solver, the learning rate doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 penalty (regularization term) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.0001\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n",
      "alpha : 0.001\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.849\n",
      "alpha : 0.1\n",
      "Accuracy on training set: 0.967\n",
      "Accuracy on test set: 0.840\n",
      "alpha : 1\n",
      "Accuracy on training set: 0.971\n",
      "Accuracy on test set: 0.853\n",
      "alpha : 10\n",
      "Accuracy on training set: 0.973\n",
      "Accuracy on test set: 0.848\n",
      "alpha : 100\n",
      "Accuracy on training set: 0.932\n",
      "Accuracy on test set: 0.868\n",
      "alpha : 1000\n",
      "Accuracy on training set: 0.906\n",
      "Accuracy on test set: 0.892\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.0001,0.001,0.1,1,10,100,1000]\n",
    "for i in alpha:\n",
    "    print(\"alpha : %s\" % i)\n",
    "    mlp = MLPClassifier(alpha=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we increase the *alpha* value, the more we reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size : (50, 50, 50)\n",
      "Accuracy on training set: 0.939\n",
      "Accuracy on test set: 0.873\n",
      "layer size : (100, 100, 100)\n",
      "Accuracy on training set: 0.950\n",
      "Accuracy on test set: 0.867\n",
      "layer size : (50, 100, 50)\n",
      "Accuracy on training set: 0.911\n",
      "Accuracy on test set: 0.889\n",
      "layer size : (100, 50, 50)\n",
      "Accuracy on training set: 0.931\n",
      "Accuracy on test set: 0.878\n",
      "layer size : (50, 50, 100)\n",
      "Accuracy on training set: 0.921\n",
      "Accuracy on test set: 0.885\n",
      "layer size : (100,)\n",
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.853\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = [(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)]\n",
    "for i in hidden_layers_size:\n",
    "    print(\"layer size : %s\" % (i,))\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = RandomizedSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n",
      "(1196, 119)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Either fit the model before transform or set \"prefit=True\" while passing the fitted estimator to the constructor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7e415ec7752a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dumps/2020.02.10-12.14.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/UCL/Memoire/masterthesis/src/machine_learning/utils.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(csv, padd, kind)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0;31m#Select best features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mtrain_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_selection/_base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         X = check_array(X, dtype=None, accept_sparse='csr',\n\u001b[1;32m     76\u001b[0m                         force_all_finite=not tags.get('allow_nan', True))\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             warn(\"No features were selected: either the data is\"\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_selection/_base.py\u001b[0m in \u001b[0;36mget_support\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36m_get_support_mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             raise ValueError('Either fit the model before transform or set'\n\u001b[0m\u001b[1;32m    176\u001b[0m                              \u001b[0;34m' \"prefit=True\" while passing the fitted'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              ' estimator to the constructor.')\n",
      "\u001b[0;31mValueError\u001b[0m: Either fit the model before transform or set \"prefit=True\" while passing the fitted estimator to the constructor."
     ]
    }
   ],
   "source": [
    "feature_selection('../dumps/2020.02.10-12.14.csv',0.15,\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
