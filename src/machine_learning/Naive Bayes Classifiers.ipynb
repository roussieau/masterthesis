{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle : Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed in the previous section. However, they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like LogisticRegression and LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from utils import feature_selection, PCA_reduction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.01.13-14.25.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, no matter of big the test size is, the performances with the Gaussian classifier are really bad for this dataset. Let's try with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.13\n",
      "Test set accuracy: 0.17\n",
      "Test set accuracy: 0.21\n",
      "Test set accuracy: 0.24\n",
      "Test set accuracy: 0.33\n",
      "Test set accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even worse ! The reaseon why this algorithm is so fast but also so bad at generalization is because it learns parameters by looking at each feature individually and collect simple per-class statistics from each feature. Since we have a huge diversity in our dataset, the GaussianNB gives quite bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Bernouilli distribution for different test sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.72\n",
      "Test set accuracy: 0.70\n",
      "Test set accuracy: 0.71\n",
      "Test set accuracy: 0.70\n",
      "Test set accuracy: 0.73\n",
      "Test set accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    \n",
    "    gnb = BernoulliNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances are not that bad but one has to know that BernouilliNB is assumes binary data (opposite to the GaussianNB which works for any kind of continuous data). We should therefore perform some tuning and only keep boolean values in our dataset. Let's scale the data to see if it improves the accuray :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 0.91\n",
      "Test set accuracy: 0.90\n",
      "Test set accuracy: 0.89\n",
      "Test set accuracy: 0.88\n",
      "Test set accuracy: 0.88\n",
      "Test set accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "    gnb = BernoulliNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive to see how great our results are now ! We can conclude that scaling is therefore quite game-changing in the case of this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the Multinomial distribution. Note that this distribution only accepts non-negative values, therefore we have either to parse our dataset and remove all rows where feature values are below 0..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3771, 119)\n",
      "Test set accuracy: 0.77\n",
      "Test set accuracy: 0.80\n",
      "Test set accuracy: 0.83\n",
      "Test set accuracy: 0.87\n",
      "Test set accuracy: 0.92\n",
      "Test set accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "for col in cols:\n",
    "    gt = gt.drop(gt[gt[col] < 0 ].index)\n",
    "data = gt[cols]\n",
    "print(data.shape)\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    \n",
    "    gnb = MultinomialNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or use some preprocessing to scale all values between 0 and 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7977, 119)\n",
      "Test set accuracy: 0.92\n",
      "Test set accuracy: 0.90\n",
      "Test set accuracy: 0.89\n",
      "Test set accuracy: 0.89\n",
      "Test set accuracy: 0.88\n",
      "Test set accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "print(data.shape)\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "\n",
    "    gnb = MultinomialNB()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : all the distribution we tested definitely need some scaling preprocessing. The Gaussian distribution didn't perform that well while Bernouilli's distribution provided the best results after normalization. The multinomial distribution quite good performances too but required removing some samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Naive Bayes assumes independence and outputs class probabilities most feature importance criteria are not a direct fit. The feature importance should be no different from the skewness of the feature distribution in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Thomas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.886\n",
      "Accuracy on test set: 0.890\n",
      "Accuracy on training set: 0.903\n",
      "Accuracy on test set: 0.901\n",
      "Accuracy on training set: 0.248\n",
      "Accuracy on test set: 0.244\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv(\"../dumps/2019-08.Merged_thomas.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "tree = GaussianNB()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))\n",
    "\n",
    "tree = BernoulliNB()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))\n",
    "\n",
    "for col in cols:\n",
    "    gt = gt.drop(gt[gt[col] < 0 ].index)\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "tree = MultinomialNB()\n",
    "\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.989\n",
      "Accuracy on test set: 0.986\n",
      "Accuracy on training set: 0.933\n",
      "Accuracy on test set: 0.930\n",
      "Accuracy on training set: 0.223\n",
      "Accuracy on test set: 0.218\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv(\"../dumps/2019-09.Merged_thomas.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "tree = GaussianNB()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))\n",
    "\n",
    "tree = BernoulliNB()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))\n",
    "\n",
    "for col in cols:\n",
    "    gt = gt.drop(gt[gt[col] < 0 ].index)\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "\n",
    "tree = MultinomialNB()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.698011    0.702142           119    0.136651\n",
      "      0.99        0.823499    0.815369            98    0.13692\n",
      "      0.95        0.95596     0.953077            77    0.131393\n",
      "      0.9         0.958085    0.955117            60    0.127462\n",
      "      0.85        0.952729    0.948657            47    0.125289\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../dumps/2020.03.11-17.39.csv','gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering features here crazily improves the accuracies from 70 to 95% in nearly the same, even slightly faster !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.920847    0.920095           119    0.142111\n",
      "      0.99        0.919912    0.915675            98    0.15339\n",
      "      0.95        0.922207    0.914655            77    0.14584\n",
      "      0.9         0.916851    0.910575            60    0.135029\n",
      "      0.85        0.918466    0.913635            47    0.136369\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../dumps/2020.03.11-17.39.csv','bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here feature clustering improves the time but slightly decreases the accuracies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
