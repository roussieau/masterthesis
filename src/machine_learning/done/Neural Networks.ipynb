{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle : In this section we'll focus on one specific type of Deep Learning algorithm, namely multilayer perceptrons. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.\n",
    "This model has a lot more coefficients (also called weights) to learn: there is one between every input and every hidden unit (which make up the hidden layer), and one between every unit in the hidden layer and the output.\n",
    "After computing a weighted sum for each hidden unit, a nonlinear function is applied to the result, usually the rectifying nonlinearity (also known as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The relu cuts off values below zero, while tanh saturates to –1 for low input values and +1 for high input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from utils import feature_selection, PCA_reduction, perf, time_comparison\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.981\n",
      "Accuracy on test set: 0.754\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.01.13-14.25.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, max_iter=10000)\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the results, we rather be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver for weight optimization.\n",
    "- ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "- ‘sgd’ refers to stochastic gradient descent.\n",
    "- ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver : lbfgs\n",
      "Accuracy on training set: 0.993\n",
      "Accuracy on test set: 0.865\n",
      "Solver : sgd\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "Solver : adam\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "solver = ['lbfgs','sgd', 'adam']\n",
    "for i in solver:\n",
    "    print(\"Solver : %s\" % i)\n",
    "    mlp = MLPClassifier(solver=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the 'adam' algorithm performs quite well on our dataset. Still, the performance on the test set might be improved by tuning other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the hidden layer.\n",
    "- ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "- ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "- ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "- ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : identity\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : logistic\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : tanh\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : relu\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "act = ['identity','logistic', 'tanh', 'relu']\n",
    "for i in act:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(activation=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the 'identity' activation didn't manage to make the algorithm converge. For the others, both 'logistic' and 'tanh' provided sam results and performed better on the test set than the training set, which is the opposite for the 'relu' activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate schedule for weight updates (only for 'sgd' solver).\n",
    "- ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "- ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : constant\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : invscaling\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : adaptive\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "learning_rate = ['constant','invscaling', 'adaptive']\n",
    "for i in learning_rate:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(solver='sgd', learning_rate=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sgd' solver, the learning rate doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 penalty (regularization term) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.0001\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n",
      "alpha : 0.001\n",
      "Accuracy on training set: 0.974\n",
      "Accuracy on test set: 0.878\n",
      "alpha : 0.1\n",
      "Accuracy on training set: 0.903\n",
      "Accuracy on test set: 0.904\n",
      "alpha : 1\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 10\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 100\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 1000\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.0001,0.001,0.1,1,10,100,1000]\n",
    "for i in alpha:\n",
    "    print(\"alpha : %s\" % i)\n",
    "    mlp = MLPClassifier(alpha=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we increase the *alpha* value, the more we reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size : (50, 50, 50)\n",
      "Accuracy on training set: 0.969\n",
      "Accuracy on test set: 0.868\n",
      "layer size : (100, 100, 100)\n",
      "Accuracy on training set: 0.980\n",
      "Accuracy on test set: 0.858\n",
      "layer size : (50, 100, 50)\n",
      "Accuracy on training set: 0.979\n",
      "Accuracy on test set: 0.870\n",
      "layer size : (100, 50, 50)\n",
      "Accuracy on training set: 0.984\n",
      "Accuracy on test set: 0.877\n",
      "layer size : (50, 50, 100)\n",
      "Accuracy on training set: 0.980\n",
      "Accuracy on test set: 0.870\n",
      "layer size : (100,)\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = [(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)]\n",
    "for i in hidden_layers_size:\n",
    "    print(\"layer size : %s\" % (i,))\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['lbfgs','sgd','adam'], 'max_iter': [1000,10000], 'alpha': [0.0001,0.001,0.1,1,10,100], 'hidden_layer_sizes':[(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)], 'activation':['identity','logistic', 'tanh', 'relu']}\n",
    "clf = RandomizedSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n",
    "clf.fit(data_train, target_train)\n",
    "print(clf.score(data_train, target_train))\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='adam',activation='tanh',alpha=100,hidden_layer_sizes=(50, 50, 100))\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='sgd',activation='tanh',alpha=0.1,hidden_layer_sizes=(100,50,50),max_iter=1000)\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between different kind of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.900\n",
      "Accuracy on test set: 0.897\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/control_8000_false_3.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.783\n",
      "Accuracy on test set: 0.771\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/default_1.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.854\n",
      "Accuracy on test set: 0.843\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/default_2.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.943\n",
      "Accuracy on test set: 0.948\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/default_4.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.943\n",
      "Accuracy on test set: 0.948\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/default_5.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors considered as positive result from detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.853\n",
      "Accuracy on test set: 0.843\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/error_as_packed.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check influence of each detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.928\n",
      "Accuracy on test set: 0.929\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/not_die.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.918\n",
      "Accuracy on test set: 0.918\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/not_cisco.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.941\n",
      "Accuracy on test set: 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/not_manalyze.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.943\n",
      "Accuracy on test set: 0.948\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/not_peid.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.943\n",
      "Accuracy on test set: 0.947\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/not_peframe.csv','mlp1',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO CONVERGENCE when we remove manalyze, quite interesting !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important : \n",
      "\n",
      "Accuracy on training set: 0.900\n",
      "Accuracy on test set: 0.897\n",
      "---------------\n",
      "\n",
      "All features : \n",
      "\n",
      "Accuracy on training set: 0.900\n",
      "Accuracy on test set: 0.897\n"
     ]
    }
   ],
   "source": [
    "perf('../dumps/control_8000_false_3.csv','mlp1',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = pd.read_csv('../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n",
      "(1196, 119)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7e415ec7752a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dumps/2020.02.10-12.14.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/UCL/Memoire/masterthesis/src/machine_learning/utils.py\u001b[0m in \u001b[0;36mfeature_selection\u001b[0;34m(csv, padd, kind)\u001b[0m\n\u001b[1;32m     83\u001b[0m \t\t\t\t                    columns=['f'+str(i) for i in range(data_test.shape[1])])\n\u001b[1;32m     84\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mbasic_stats_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mbasic_stats_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "feature_selection('../dumps/2020.02.10-12.14.csv',0.15,\"mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with Thomas datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.769\n",
      "Accuracy on test set: 0.761\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv(\"../dumps/2019-08.Merged_thomas.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "tree = MLPClassifier(solver='adam',activation='tanh',alpha=100,hidden_layer_sizes=(50, 50, 100))\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.784\n",
      "Accuracy on test set: 0.787\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv(\"../dumps/2019-09.Merged_thomas.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "tree = MLPClassifier(solver='adam',activation='tanh',alpha=100,hidden_layer_sizes=(50, 50, 100))\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.858357    0.857531           119    14.8017\n",
      "      0.99        0.858357    0.857531            98     9.46293\n",
      "      0.95        0.858357    0.857531            77     8.73768\n",
      "      0.9         0.858357    0.857531            60     8.37715\n",
      "      0.85        0.858357    0.857531            47     8.2692\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../dumps/2020.03.11-17.39.csv','mlp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test was conducted with the following caracteristics:\n",
    "- MLPClassifier(solver='adam',activation='tanh',alpha=100,hidden_layer_sizes=(50, 50, 100))\n",
    "\n",
    "which were obtained by selecting the best results from the previous tests before normalization. As we can see, overall accuracies are not really satisfying. We can also observe what we can observe for nearly every PCA, pretty same accuracies with less components in half the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variance    Training acc    Test acc    Components    Time (s)\n",
      "----------  --------------  ----------  ------------  ----------\n",
      "      1           0.992348    0.978239           119     84.9651\n",
      "      0.99        0.992263    0.978239            98     72.8201\n",
      "      0.95        0.991583    0.977899            77     48.1287\n",
      "      0.9         0.991498    0.976879            60     74.0656\n",
      "      0.85        0.989628    0.979259            47     42.5804\n"
     ]
    }
   ],
   "source": [
    "PCA_reduction('../dumps/2020.03.11-17.39.csv','mlp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters here were chosen after normalization leading to this configuration : \n",
    "- MLPClassifier(solver='sgd',activation='tanh',alpha=0.1,hidden_layer_sizes=(100,50,50),max_iter=1000)\n",
    "\n",
    "which offer really good performance and the training set for less components and half computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time fiability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['snapshots/mlp_default_20190615_6000.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = pd.read_csv(\"../dumps/default_20190615_6000.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data_train = gt[cols]\n",
    "target_train = gt['label']\n",
    "\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd',activation='tanh',alpha=0.1,hidden_layer_sizes=(100,50,50),max_iter=1000)\n",
    "clf.fit(data_train, target_train)\n",
    "dump(clf,\"snapshots/mlp_default_20190615_6000.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.919\n",
      "Accuracy on test set: 0.897\n"
     ]
    }
   ],
   "source": [
    "clf = load(\"snapshots/mlp_default_20190615_6000.joblib\")\n",
    "\n",
    "gt = pd.read_csv(\"../dumps/default_20190808_1000.csv\")\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data_test = gt[cols]\n",
    "target_test = gt['label']\n",
    "\n",
    "data_test = scaler.transform(data_test)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(clf.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(clf.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : we can assess that performances are kept in a reasonable range over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptation threshold : 1/5 \n",
      "\n",
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.7965         0.691\n",
      "                       14000                   2               0.782857       0.691\n",
      "                       21000                   3               0.754286       0.691\n",
      "                       31000                   4.42857         0.689258       0.691\n",
      "Acceptation threshold : 2/5 \n",
      "\n",
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.868833       0.855\n",
      "                       14000                   2               0.856286       0.855\n",
      "                       21000                   3               0.827905       0.855\n",
      "                       31000                   4.42857         0.760581       0.855\n",
      "Acceptation threshold : default/5 \n",
      "\n",
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.905333       0.896\n",
      "                       14000                   2               0.902714       0.896\n",
      "                       21000                   3               0.889619       0.896\n",
      "                       31000                   4.42857         0.882935       0.896\n",
      "Acceptation threshold : 4/5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.953167        0.94\n",
      "                       14000                   2               0.947786        0.94\n",
      "                       21000                   3               0.938952        0.94\n",
      "                       31000                   4.42857         0.938935        0.94\n",
      "Acceptation threshold : 5/5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.953167        0.94\n",
      "                       14000                   2               0.947786        0.94\n",
      "                       21000                   3               0.938952        0.94\n",
      "                       31000                   4.42857         0.938935        0.94\n"
     ]
    }
   ],
   "source": [
    "time_comparison('mlp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptation threshold : 1/5 \n",
      "\n",
      "  # malwares in training set    Approx. period in weeks    Training acc    Test acc\n",
      "----------------------------  -------------------------  --------------  ----------\n",
      "                        6000                   0.857143        0.821333       0.709\n",
      "                       14000                   2               0.799571       0.72\n",
      "                       21000                   3               0.801667       0.795\n",
      "                       31000                   4.42857         0.761839       0.807\n",
      "Acceptation threshold : 2/5 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "time_comparison('mlp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same conclusion as linear models, by increasing the training set, the performances on the test set do not necessarily improve, it's quite constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
