{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle : Naive Bayes classifiers are a family of classifiers that are quite similar to the linear models discussed in the previous section. However, they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide generalization performance that is slightly worse than that of linear classifiers like LogisticRegression and LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.22\n",
      "Test set accuracy: 0.12\n",
      "Training set accuracy: 0.26\n",
      "Test set accuracy: 0.18\n",
      "Training set accuracy: 0.28\n",
      "Test set accuracy: 0.20\n",
      "Training set accuracy: 0.28\n",
      "Test set accuracy: 0.20\n",
      "Training set accuracy: 0.24\n",
      "Test set accuracy: 0.21\n",
      "Training set accuracy: 0.36\n",
      "Test set accuracy: 0.31\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/1K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Training set accuracy: {:.2f}\".format(gnb.score(data_train, target_train)))\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in general, the performances with the Gaussian classifier are really bad for this dataset. Let's try with more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.65\n",
      "Test set accuracy: 0.61\n",
      "Training set accuracy: 0.67\n",
      "Test set accuracy: 0.61\n",
      "Training set accuracy: 0.73\n",
      "Test set accuracy: 0.68\n",
      "Training set accuracy: 0.76\n",
      "Test set accuracy: 0.69\n",
      "Training set accuracy: 0.80\n",
      "Test set accuracy: 0.78\n",
      "Training set accuracy: 0.99\n",
      "Test set accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/1K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Training set accuracy: {:.2f}\".format(gnb.score(data_train, target_train)))\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same problem ! The reaseon why this algorithm is so fast but also so bad at generalization is because it learns parameters by looking at each feature individually and collect simple per-class statistics from each feature. Since we have a huge diversity in our dataset, the GaussianNB gives quite bad results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the Bernouilli distribution for different test sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.89\n",
      "Test set accuracy: 0.89\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.87\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.87\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.87\n",
      "Training set accuracy: 0.91\n",
      "Test set accuracy: 0.89\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/1K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    \n",
    "    gnb = BernoulliNB()\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Training set accuracy: {:.2f}\".format(gnb.score(data_train, target_train)))\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances are not that bad but one has to know that BernouilliNB is assumes binary data (opposite to the GaussianNB which works for any kind of continuous data). We should therefore perform some tuning and only keep boolean values in our dataset. Let's scale the data to see if it improves the accuray :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.87\n",
      "Test set accuracy: 0.86\n",
      "Training set accuracy: 0.87\n",
      "Test set accuracy: 0.84\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.85\n",
      "Training set accuracy: 0.90\n",
      "Test set accuracy: 0.87\n",
      "Training set accuracy: 0.91\n",
      "Test set accuracy: 0.90\n",
      "Training set accuracy: 0.98\n",
      "Test set accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/1K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "    gnb = BernoulliNB(binarize=0.0)\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Training set accuracy: {:.2f}\".format(gnb.score(data_train, target_train)))\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressive to see how great our results are now ! We can conclude that scaling is therefore quite game-changing in the case of this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the Multinomial distribution. Note that this distribution only accepts non-negative values, therefore we have either to use some preprocessing to scale all values between 0 and 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 119)\n",
      "Training set accuracy: 0.91\n",
      "Test set accuracy: 0.93\n",
      "Training set accuracy: 0.91\n",
      "Test set accuracy: 0.92\n",
      "Training set accuracy: 0.91\n",
      "Test set accuracy: 0.91\n",
      "Training set accuracy: 0.92\n",
      "Test set accuracy: 0.90\n",
      "Training set accuracy: 0.94\n",
      "Test set accuracy: 0.92\n",
      "Training set accuracy: 0.96\n",
      "Test set accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "gt = pd.read_csv('../../dumps/various_sizes/1K.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "print(data.shape)\n",
    "target = gt['label']\n",
    "\n",
    "values = [0.1,0.2,0.4,0.6,0.8,0.9]\n",
    "\n",
    "for i in values:\n",
    "    data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = i, random_state = 0)\n",
    "\n",
    "    gnb = MultinomialNB()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(data_train)\n",
    "    data_train = scaler.transform(data_train)\n",
    "    data_test = scaler.transform(data_test)\n",
    "\n",
    "    gnb.fit(data_train, target_train)\n",
    "    print(\"Training set accuracy: {:.2f}\".format(gnb.score(data_train, target_train)))\n",
    "    print(\"Test set accuracy: {:.2f}\".format(gnb.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : all the distributions we tested definitely need some scaling preprocessing. The Gaussian distribution didn't perform that well while Bernouilli's distribution provided the best results after normalization. The multinomial distribution quite good performances too but values are scaled then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
