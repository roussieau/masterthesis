{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from utils import perf, thomas_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solver for weight optimization.\n",
    "- ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "- ‘sgd’ refers to stochastic gradient descent.\n",
    "- ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver : lbfgs\n",
      "Accuracy on training set: 0.993\n",
      "Accuracy on test set: 0.865\n",
      "Solver : sgd\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "Solver : adam\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "solver = ['lbfgs','sgd', 'adam']\n",
    "for i in solver:\n",
    "    print(\"Solver : %s\" % i)\n",
    "    mlp = MLPClassifier(solver=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could expect, the 'adam' algorithm performs quite well on our dataset. Still, the performance on the test set might be improved by tuning other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the hidden layer.\n",
    "- ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "- ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "- ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "- ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : identity\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : logistic\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : tanh\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : relu\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "act = ['identity','logistic', 'tanh', 'relu']\n",
    "for i in act:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(activation=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the 'identity' activation didn't manage to make the algorithm converge. For the others, both 'logistic' and 'tanh' provided sam results and performed better on the test set than the training set, which is the opposite for the 'relu' activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate schedule for weight updates (only for 'sgd' solver).\n",
    "- ‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "- ‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "- ‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function : constant\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : invscaling\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "function : adaptive\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "learning_rate = ['constant','invscaling', 'adaptive']\n",
    "for i in learning_rate:\n",
    "    print(\"function : %s\" % i)\n",
    "    mlp = MLPClassifier(solver='sgd', learning_rate=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'sgd' solver, the learning rate doesn't really matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 penalty (regularization term) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.0001\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n",
      "alpha : 0.001\n",
      "Accuracy on training set: 0.974\n",
      "Accuracy on test set: 0.878\n",
      "alpha : 0.1\n",
      "Accuracy on training set: 0.903\n",
      "Accuracy on test set: 0.904\n",
      "alpha : 1\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 10\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 100\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n",
      "alpha : 1000\n",
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "alpha = [0.0001,0.001,0.1,1,10,100,1000]\n",
    "for i in alpha:\n",
    "    print(\"alpha : %s\" % i)\n",
    "    mlp = MLPClassifier(alpha=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more we increase the *alpha* value, the more we reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter allows us to set the number of layers and the number of nodes we wish to have in the Neural Network Classifier. Each element in the tuple represents the number of nodes at the ith position where i is the index of the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size : (50, 50, 50)\n",
      "Accuracy on training set: 0.969\n",
      "Accuracy on test set: 0.868\n",
      "layer size : (100, 100, 100)\n",
      "Accuracy on training set: 0.980\n",
      "Accuracy on test set: 0.858\n",
      "layer size : (50, 100, 50)\n",
      "Accuracy on training set: 0.979\n",
      "Accuracy on test set: 0.870\n",
      "layer size : (100, 50, 50)\n",
      "Accuracy on training set: 0.984\n",
      "Accuracy on test set: 0.877\n",
      "layer size : (50, 50, 100)\n",
      "Accuracy on training set: 0.980\n",
      "Accuracy on test set: 0.870\n",
      "layer size : (100,)\n",
      "Accuracy on training set: 0.977\n",
      "Accuracy on test set: 0.873\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = [(50,50,50), (100,100,100), (50,100,50), (100,50,50), (50,50,100), (100,)]\n",
    "for i in hidden_layers_size:\n",
    "    print(\"layer size : %s\" % (i,))\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=i, random_state=0, max_iter=10000) \n",
    "    mlp.fit(data_train, target_train)\n",
    "    print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "    print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = pd.read_csv('../../dumps/2020.02.10-12.14.csv')\n",
    "cols = [col for col in gt.columns if col not in ['label']]\n",
    "data = gt[cols]\n",
    "target = gt['label']\n",
    "data_train, data_test, target_train, target_test = train_test_split(data,target, test_size = 0.20, random_state = 0)\n",
    "scaler = Normalizer()\n",
    "scaler.fit(data_train)\n",
    "data_train = scaler.transform(data_train)\n",
    "data_test = scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='adam',activation='tanh',alpha=100,hidden_layer_sizes=(50, 50, 100))\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.895\n",
      "Accuracy on test set: 0.900\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(solver='sgd',activation='tanh',alpha=0.1,hidden_layer_sizes=(100,50,50),max_iter=1000)\n",
    "mlp.fit(data_train, target_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(mlp.score(data_train, target_train))) \n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(data_test, target_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
